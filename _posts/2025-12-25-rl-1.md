---
layout: post
title: RL 공부 - 1
date: 2025-12-25 00:00:00
permalink: /:title
categories: [RL, AI]
tag: ['2026']
description: Intro
comments: true
math: true
published: true
lang: ko
---

# DRL이 뭐길래

어떤 시스템은 주어진 정보에 따라 정보를 받고 관찰하고, 이에 따른 행동을 하고.. 이를 반복해야 하고, 여기에 대한 해결책이
- imitation learning
- offline & online RL
- model-free * model-based RL
- multi-task & meta RL
- RL for robots
- RL of LLMs

등 다양하고, 해당 과목은 DNN으로 scale되는 solution들에 집중한다. 

# RL이 다른 ML 주제와 다른 점?

지도학습의 경우 우리가 $\{(x,y)\}$ dataset을 받고 좋은 $f(x) \sim y$를 찾는 거였는데, $x$는 iid한 distribution에서 왔고 우리는 뭘 출력해야하는지를 아는 반면, 

RL은 $\pi(a\vert s)$을 경험과 간접피드백을 통해서 학습하려 하며, data는 i.i.d가 아닐 수 있다. (즉, $a$: action이 미래의 observation에 영향을 준다)

# DRL이 필요한 이유

크게 보자면 다음의 4가지다. 
- 지도학습의 $(x,y)$을 넘어서기
- AI system에 매우 광활히 사용됨
- 지능에 대한 상당히 fundamental한 approach
- open research problem이 많음

## 지도학습의 $(x,y)$을 넘어서기

Decision-making problem은 모든 곳에 있다!
- 어떤 종류든지 간에, AI agent들: 로봇, 자율주행, 웹도우미
- AI system이 사람과 소통할 시
- 시스템이 미래의 outcome, observation에 영향을 미칠 시
- label이 없거나, 단순히 정확도가 목적이 아닐 때 (+미분 불가능?)

## AI system에 매우 광활히 사용됨

여러 복잡한 물리적 과제(걷기만 해도 어려웠다), 복잡한 게임(바둑 등), LLM, 교통 제어 등 굉장히 많은 분야에서 이용된다. 게다가 칩 설계에도 사용되었다!

## 지능에 대한 상당히 fundamental한 approach

연습을 통해 나아지는 건 사람도 마찬가지. 

## open research problem이 많음

- 이 task에 대해 무엇이 좋은지 안 좋은지를 알 수 있나? $\rightarrow$ reward learning
- 이 task를 다양한 시나리오로 일반화시킬 수 있나? $\rightarrow$ offline RL, multitask RL, meta-RL
- RL이 오래 걸리는 과제에 사용될 수 있나? $\rightarrow$ hierachy, reasoning
- 학습의 자동화가 가능한가? $\rightarrow$ reset-free RL

등등의 문제로 이어진다. 

# RL 기초

기본적인 개념부터 집고 가자. 
- $s_t$: 시간 $t$에서의 world의 상태
- $o_t$: agent가 시간 $t$에서 관찰하는 것(정보가 부족한 경우 사용)
- $a_t$: 시간 $t$에 선택한 decision
- Trajectory $\tau$: 택한 state/observation와 action의 sequence: $(s_1,a_1,\cdots,s_T,a_T)$
- reward function $r(s,a)$: $s,a$가 얼마나 좋은지?

한 가지 눈여겨볼만 한 점은, next state는 오로지 현재 state와 그의 action(그리고 randomness: 그래서 확률분포로써 표현됨)에 대한 함수로 나온다는 것이다. 즉, $p(s_{t+1}\vert s_t,a_t)$ 이 $s_{t-1}$ 와는 독립적이라는 것이고, 이를 **Markov Property**라고 부른다. 

당연히 다음 질문은 현재 $s$에서 어떻게 $a$를 획득하는지에 대한 부분인데, 이는 policy $\theta$가 담당한다. 

즉 
1. $s_t$를 관찰하고
2. $a_t$를 취하고 ($\pi_{\theta}(\cdot \vert s_t)$에서 sampling해서)
3. 그다음 state $s_{t+1}$를 관찰하기 (world dynamics $p(\cdot \vert s_t, a_t)$에서 sample해서)

이의 결과는 Trajectory일 것이다. (혹은 policy roll-out, episode라고 말한다)

만일 observation만 가진다면, policy에 대해 memory를 $\pi_{\theta}(a_t\vert o_{t-m},\cdots,o_{t})$로 $m$ step 전의 메모리를 retain한 걸로 policy를 잡을 수도 있다. 

그럼 이 policy의 궁극적 목표는 무엇일까? 

그건 reward의 합을 최대화하는 것인데, 

$\max\sum\limits_{t}^{T}r(s_t,a_t)$는 deterministic하지 않다. 
1. state 고르는 것에 randomness가 있고(world가 stochastic하다)
2. policy 자체가 nondeterminstic일 수도 있음. 

주어진 policy에 대해, 특정 trajectory이 확정적이 나오는 것이 아닌 것이다. 

$p_{\theta}(\tau)=p(s_1,a_1,\cdots,s_T,a_T)=p(s_1)\prod\limits_{t=1}^{T}\pi_{\theta}(a_t \vert s_t)p(s_{t+1} \vert s_t,a_t)$ 으로 볼 수 있겠다. 

그렇다면 이 확률분포 아래서, reward의 기댓값을 maximize하는 것으로 볼 수가 있는데, 

즉
$\max\limits_{\theta}\mathbb{E}_{\tau \sim p\_{\theta}(\tau)}\left[\sum\limits\_{t}^{T}r(s\_t,a\_t)\right]$
를 구하는 것이다.

왜 stochastic policy를 쓰냐면
1. 탐색: 여러 가지 policy를 해봐야니..
2. 아무래도 현재 주어진 data는 여러 가지 behavior를 보일 수 있으니

그래서 generative modeling에서의 도구를 가져올 수도 있다. 즉, 주어진 state/observation에 대해서 action들에 대해 generative model를 쓰는 것. 

아무래도 더 좋은 policy를 찾고 싶다면, 현재 policy를 evaluate해야 하는 방법도 필요하다. 여기는 Value function $V^{\pi}(s)$, Q function $Q^{\pi}(s,a)$이 있다. 자세한 건 다음 시간에...

쨌든 이 expected sum of reward를 늘리고 싶다면 뭘 해야 할까?
- Imitation Learning: reward가 높은 정책 따라하기
- Policy Gradients: 그냥 위에껄 그냥 미분에서 gradient descent하면 되는 거 아님?
- Actor-critic: 현재 policy의 value를 예측해서 이걸로 policy를 낫게 하기
- Value-based: optimal policy의 value 예상하기
- Model-based: dynamics의 model를 배워서, planning이나 policy improvement에 쓰기

등의 방식이 있다.

가능한 알고리즘이 꽤나 많은데, 그 이유는 여러 assumption에 대한 여러 trade-off(data를 얻는 속도, 관리의 속도, 안정성, dimensionality, continuous vs discrete, dynamics model을 배우기 쉬운지 등등..)가 존재하기 때문. 

그래서 현재까지의 과정을 state를 보고 하면 MDP(Markov Decision Process), Observation을 보고 하면 POMDP(Partially-Observed Markov Process)로 정의한다. 
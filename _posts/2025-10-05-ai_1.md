---
layout: post
title: Learning Theory 공부 - 1
date: 2025-10-05 00:00:00
permalink: /:title
categories: [AI]
tag: ['AI','Mathematics']
description: Learning Theory 공부하기
comments: true
math: true
published: false
lang: ko
---

> **작성중인 글입니다. 내용이 추가될 예정입니다.**


오랜만에 복귀하였습니다. 시험 공부도 할 겸, learning theory 복습 좀 합시다. 책 내용을 따라가긴 한데 렉쳐에서 언급한 건 과감하게 컷. 

# 1. Mathematical Preliminaries
Learning Theory을 위한 알고리즘 분석을 위한 분석의 시작이다. 

## 1.1 Linear Algebra and Differentiable Calculus
이후에 다양하게 사용될 선형대수/미적분 개념을 을 복습해보자.
### 1.1.1. Minimization of Quadratic Froms
### 1.1.2 Inverting a $2 \times 2$ matrix
### 1.1.3. Inverting Matrices Defined by Blocks, Matrix Inversion Lemma
### 1.1.4. Eigenvalue and SVD
### 1.1.5. Differentiable Calculus
## 1.2 Concentration Inequalities
### 1.2.1. Hoeffding's Inequality
### 1.2.2. McDiarmid's Inequality
### 1.2.3. Bernstein's Inequality
### 1.2.4. Expectation of the Maximum
### 1.2.6. Concentration Inequalities for Random Matrices

# 2. Introduction to Supervised Learning
진짜 학습의 시작. SL부터 들어가자. 

## 2.1. From Training Data to Predictions
SL의 setup은 training data($x$값에 따른 $y$값의 모음집)이 있고, 이를 바탕으로 learner가 $f:\mathcal{X} \rightarrow \mathcal{Y}$인 좋은 predictor를 찾아서, 보지 않은 $x$에 대해서 $y$를 잘 예측하는 것이 목표이다. 

하지만 SL은 생각보단 어려운데, 
- Noise/Randomness: $(x,y)$는 랜덤하고, 특정 deterministic한 mapping이 존재하지 않는다. 예를 들어, $x$에 대해 의존적인 noise가 껴있을 수 있음. 
- Complex $f$: 고차원으로 가면 높은 확률로 비선형이다. 
- Finite-sample: 우리는 적은 양의 sample만 있고, 이는 추후의 interpolation/extrapolation에 연결된다. 좀 더 자세하게 말하자면, interpolation는 보여준 data의 range 안에서 예측하는 거, extrapolation는 밖에서 예측하는 거다. (이게 더 힘듦) 
- High dimensionality: input space가 고차원이면 curse of dimensionality를 마주하게 된다.

이게 뭔 의미냐면, input space가 고차원이라고 한다면, 우리는 $n$의 sample을 바탕으로 estimate할 때 $x$와 그래도 가까운 게 하나는 있으면 좋을 것이다. 그럼 $n$을 잘 잡을 때 가장 가까운 한 점과의 거리가 차원에 대해 기하급수적으로 증가함.. 
- Distributional shift: train data와 test data가 서로 다른 분포에서 올 수도 있다. 

## 2.2. Decision Theory
일단은 확률적인 framework에서 시작한다. 
- Training data는 $p(x,y)$ 분포를 따르고, 
- Test data는 같은 분포에서 오고, 
- Learning algorithm $\mathcal{A}$는 우리의 dataset을 predictor로 mapping하는 함수이다. 
그렇다면, 이 알고리즘이 좋다는 걸 어떻게 판정할까? Loss, Risk, Bayes predictor를 들고 온다.

### 2.2.1. Supervised Learning Problems and Loss Functions
$l: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$는 loss function으로, 실제값이 $y$일 때 $z$를 예측할 때 주는 penalty이다. 예시로는
- Binary classification: $l(y,z)=1_{y \neq z}$, 즉 맞추면 1 아니면 0. (0-1 loss)
- Multiclass: 결국 Binary classification에 비해 $\mathcal{Y}=\{1,\cdots,k\}$로 확장된 거..
- Regression: $\mathcal{Y}=\mathbb{R}$, $l(y,z)=(y-z)^{2}$ (squared loss), 혹은 robust alternative로 absolute loss를 쓰기도 한다. 

어떤 경우에는 Training은 더 좋은 optimization property를 가진 비슷한 surrogate loss를 minimize하는 경우도 있다. (ex. cross-entropy/hinge를 0-1에 대한 surrogate로 쓰기..)

### 2.2.2. Risks
이제, Risk를 정의해보자. $\mathcal{R}(f)=\mathbb{E}[l(y,f(x))]$로, predictor가 주어질 때 모든 가능한 loss의 기댓값을 취한 거다. 하지만 여기서 문제점은 우리는 finite한 dataset한테만 의존하는데, 그렇기에 Empirical Risk $\hat{\mathcal{R}}(f)=\frac{1}{n}\sum\limits_{i=1}^{n}l(y_{i},f(x_{i}))$을 쓰는 거가 강제된다. 예를 들자면,
- Binary 0-1 loss에 대해, $\mathcal{R}(f)=\mathbb{P}(f(x) \neq y)$, 걍 loss가 indicator function이니까..
- Regression square loss에 대해, $\mathcal{R}(f)=\mathbb{E}[(y-f(x))^{2}]$, 나중에 다룰 꺼지만 MSE라는 명칭이 있음.  

### 2.2.3. Bayes Risk and Bayes Predictor
Tower rule에 대해서 $x$를 고정시켰을 때의 risk의 평균에 대해, $x$에 대한 기댓값을 구해주면 그냥 risk가 나온다. 그렇다면 conditional risk를 $r(z|x')=\mathbb{E}[l(y,z)\mid x=x']$로 정의해주면, $\mathcal{R}(f)=\int_{\mathcal{X}}r(f(x')\mid x')dp(x')$ 로 볼 수 있다. 

이제 우리는 아무 minimizer가 $f^{\ast}(x') \in \text{argmin}_{z \in \mathcal{Y}}\mathbb{E}[l(y,z)\mid x=x']$ 를 모든 $x' \in \mathcal{X}$에 대해 만족시킨다면, 이를 Bayes predictor라 하자. 결론적으로, 모든 가능한 $x$에 대해서, loss를 최소하는 값을 mapping된 값으로 고르면 Bayes predictor인 것이다. 

Bayes risk는 
$\mathcal{R}^{\ast}=\mathbb{E}_{x}[\inf_{z \in \mathcal{Y}}\mathbb{E}[l(y,z)\mid x]]$

로 볼 수 있는데, 이는 위에서 conditional risk로 정의한 risk 식에 대입하면 

$\mathcal{R}(f) - \mathcal{R}^{\ast} \ge 0$ 
라는 결론이 나온다.


몇 가지 중요한 특징이 있는데, 
- Bayes predictor는 unique하진 않을 수도 있긴 한데, 모두 Bayes risk는 동일하게 달성한다. 
- Irreducible error: 일반적으로 $\mathcal{R}^{\ast}>0$ 일 수 밖에 없고, 그게 아닌 경우는 deterministic 한 경우 말곤 없다. 

마지막으로 excess risk를 $\mathcal{E}(f)=\mathcal{R}(f')-\mathcal{R}^{\ast}$ 로 정의하고 넘어가자. 한 가지는 우리가 distribution을 안다면 우리는 Bayes predictor를 직접 구할 수도 있겠다만, 당연히 전제는 우리는 $p$ 를 모른다는 것이다. 

Binary classification를 예로 들어보자. $\rho(x)=\mathbb{P}[y=1\mid x]$ 을 regression function로 정의한다면, conditional risk는 $r_{x}(z)=\mathbb{E}[l(y,z) \mid x]=1-\mathbb{P}[y=z \mid x]$이 될 건데(서로 다를때 패널티가 1이니) 즉 저 확률을 최대화시키면 되는 일인데, 최소로 하는 방법은 간단하다: $\rho(x),1-\rho(x)$ 중 최소를 선택하게 설정하고, 이거에 대해 기댓값을 구해주면 된다. 

이번에는 squared loss 사례를 보자. $r_{x}(z)=\mathbb{E}[(y-z)^{2} \mid x]=\mathbb{E}[(y-\mu(x))^{2}\mid x]+(\mu(x)-z)^{2}$ 임을 식 정리하면서 확인할 수 있는데, 결국 앞부분은 고정이니 $z=\mu(x)=\mathbb{E}[y \mid x]$ 로 설정하는 것이 최적이고, 이때 conditional risk는 $\text{Var}(y \mid x)$ 일 것이며, 이것에 대해 $x$ 에 expectation을 취하면 Bayes risk가 나올 것이다. 

## 2.3. Learning from Data

당연히 목표는 이제 Bayes predictor로 향하는 것이 목표이다. 

### 2.3.1. Local Averaging

한 가지 아이디어는 kNN이다. 즉, 주어진 test point에 대해, 가장 가까운 $k$ 개의 training input을 골라서, regression이면 평균, classification이면 다수결로 정하는 것이다. (어째보면 아까 한 Bayes estimator을 추정하는 방법..)

장점으로는 전혀 training/optimization 같은 게 필요없고, 구현이 매우 쉽다는 점이 있고, 특히 저차원, 비선형 상황에서 제 힘을 발휘한다. 문제는 쿼리 시간이 좀 걸리고, curse-of-dimensionality에서 고차원에 약하고, $k$과 거리의 선택이 매우 중요해진다. $k$가 매우 크다면 underfitting(거의 상수 predictor)가 나올 것이고, $k$가 너무 작으면 overfitting이 있을 것이다. (train-test 차이가 큼)

### 2.3.2. Empirical Risk Minimization

한 가지 생각은 이제, function class를 $\mathcal{F}=\{f_{\theta}\mid \theta \in \Theta\}$로 잡은 후에, empirical risk를 가능한 모든 함수 중에서 최소화하는 $\theta$로 잡으면 어떨까라는 생각에 도달하게 된다. 그럼 우리는 ERM predictor $f_{\hat{\theta}}$에 대해,  $\mathcal{R}(f_{\hat{\theta}})-\mathcal{R}^{\ast}$를 $\mathcal{R}(f_{\hat{\theta}})-\inf_{\theta \in \Theta}\mathcal{R}(f_{\theta})$ (Estimation error: $n$이 커질 수록 작아지겠지만 $\Theta$의 크기에 대해 커짐) 과 $\inf_{\theta \in \Theta}\mathcal{R}(f_{\theta})-\mathcal{R}^{\ast}$ (Approximation Error: 단순히 $\Theta$에 달려 있음)로 excess risk를 쪼갤 수 있다. 

앞에서 말한 Underfitting와 Overfitting의 특징을 다시금 정리해본다면

Underfitting
- Training error가 큼. 
- Function class가 너무 작음. 
- Bias가 큰 대신, variance가 작음

Overfitting
- Training error는 작은데, test error가 큼. 
- Noise를 memorize함. ($\mathcal{F}$가 너무 큼..)
- Bias가 낮은 대신, Variance가 높음. 

이게 일반적인 생각이긴 한데, 나중에 double descent라는 획기적 무기를 들고와서 오히려 모델 사이즈를 키워도 된다는 결론에 도달하긴 할꺼다..

추가적 관찰로는, $\theta'$를 population risk에서 $\Theta$ 상 minimizer라 해보자. 몇 번의 더하기 빼기를 통해 $\mathcal{R}(f_{\hat{\theta}})-\mathcal{R}(f_{\theta'}) \leq 2 \sup\limits_{\theta \in \Theta}|\hat{\mathcal{R}}(f_{\theta})-\mathcal{R}(f_{\theta})|+\hat{\mathcal{R}}(f_{\hat{\theta}})-\hat{\mathcal{R}}(f_{\theta}')$임을 알 수 있다. 앞 부분의 경우 generalization term이라면, 두 번째 항의 경우는 optimization inaccuracy를 잰다. 

추후에 Linear regression에서 계속 보겠지만, 어떤 경우는 overfitting을 해서하기 위해서, regularization term을 추가하는 경우가 있다. 이는, 오히려 그렇게 penalized된 objective가 optimize하기는 더 쉽기 때문이다. 자세한 건 나중에 ridge regression에서 확인하자. 

## 2.4. Statistical Learning Theory

용어를 몇 개 다시 보자. Training data를 $D_{n}(p)$ (즉 $p$ 분포를 따르는 $n$개에 대해..) Learning algorithm $\mathcal{A}$는 $\mathcal{A}(D_{n}(p))=f_{\hat{\theta}}$, 그러니까 주어진 data를 바탕으로 ERM predictor를 주는 알고리즘이다. 

### 2.4.1. Measures of Performance

그렇다면, 당연히 목표는 small excess risk를 가지는 것이다. 물론 data에 대해서 결과가 다를 것이니, 기댓값을 취하자. 즉 Expected error를 $\mathbb{E}[\mathcal{R}(\mathcal{A}(D_{n}(p)))]-\mathcal{R}_{p}^{\ast}$로 보자. 이때, 우리는 $n \rightarrow \infty$일 때 이 expected error가 $0$으로 수렴한다면, 알고리즘을 consistent하다고 정의할 것이다. 여기서도 PAC의 정의를 가져올 수 있는데, 비슷한 표현으로 $\mathbb{P}[\mathcal{R}(\mathcal{A}(D_{n}(p)))-\mathcal{R}_{p}^{\ast} \leq \varepsilon] \ge 1-\delta$ 라는, 거롤 해서, $\varepsilon=\varepsilon(n,\delta)$로 최소화하는 것을 목표로 할 것이다. 

### 2.4.2. Notions of Consistency over Classes of Problems 

Uniform consistency의 경우는 단순히 한 분포 $p$가 아닌, class $\mathcal{P}$에 대해 여기서 속한 분포들에 대해 확장하여 supremum이 $0$으로 수렴한다고 말할 것이다. Universal consistency는 모든 분포에 대해 uniform하다는 건데...이게 가능할까? 

이 논의는 뒤에서 하고, 우리가 아까 했던 uniform consistency가 class $\mathcal{P}$에 대해 된다고 했을 때, 한 가지 생각은 여기서 알고리즘 $\mathcal{A}$에 대해 inifimum을 취하는 건데, 이를 minimax excess risk라고 부르자. 만일 upper bound라는 걸 보는 건 알고리즘을 분석하는 것이고, lower bound를 본다면 무조건 어떠한 알고리즘이 해당만큼은 감수해야 한다는 의미일 것이다. 만일 둘이 상수배 차이라면, 해당 rate는 $\mathcal{P}$에서 최적이라는 것. 


## 2.5. No Free Lunch Theorems

아까 했던 질문의 정답은 불가능. Assumption은 필수다. 그러니까, NFL Theorem에 알려주는 사실은 다음과 같다: 
- binary classification이 0,1 loss, $\mathcal{X}$은 무한하다고 가정하고
- $\mathcal{P}$를 모든 가능한 $\mathcal{X} \times \{0,1\}$라 할 때, 어떠한 학습 알고리즘에 대해서,도 expected error의 supremum은 1/2 이상이라는 결과가 나온다. 그러니까, 그냥 찍기만도 못하다..라는 distribution 또한 존재한다는 의미. 

증명은 간단하게 설명하자면, 좋은 deterministic한 distribution을 바탕으로 supremum의 lower bound를 잡고, 그거가 1/2 이상임을 보일 수가 있다. 

그렇다면, 당연히 좋은 rate를 얻기 위해서는 여러 조건(Lipschitzness, Smoothness, Low effective dimension) 등의 방안으로 조건을 부여하는 것이 가능할 것이다. 


# 3. Linear Least-Square Regression

최고로 중요하고 기본 중에서 기본인 Linear Regression으로 드가자. 

## 3.1 Introduction

Gauss, Legendre 있던 시절부터 있었고, bias-variance tradeoff/generalization property 잘 보여주고, closed form solution, easy analysis, flexibility(nonlinear feature들의 선형결합으로 complex function을 찾아낼 수 있음!!)

## 3.2 Least-Squares Framework

2단원 내용 복습을 좀 하자. 우리는 현재 training data르르 바탕으로 regression을 할 꺼고, $y \in \mathbb{R}$, model을 $f_{\theta}(x)(\theta \in \Theta)$로 볼 것이며, 이때 ERM을 시켜주는 걸 $\hat{\theta}$로 잡을 것이고, 어떤 경우에는 Bayes predictor가 아예 이 model에 없을 수도 있다. 이런 경우 우리는 model을 misspecified라고 한다. 

$\theta$에 대해서 linear하다는 건, $f_{\theta}(x)=\varphi(x)^{\intercal}\theta$ 로 표현할 수 있다는 것, 이때 $\varphi(x) \in \mathbb{R}^{d}$를 feature vector이라고 하자. Bias의 경우는 $1$이라는 constant feature을 append함으로써 관리할 수 있다. 어쨌든, 이 방식의 중요한 점은 linearity를 $\theta$입장에서 유지하면서, 상당히 richer space로 input로 확장할 수 있다는 점에 있다. 

Empirical Risk를 그래서 Matrix $\Phi \in \mathbb{R}^{n \times d}$로 표현한다면, $\hat{\mathcal{R}}(\theta)=\frac{1}{n}\|y-\Phi\theta\|_{2}^{2}$으로 볼 수 있다. 

## 3.3 Ordinary Least-Squares Estimator

만일 $\Phi$가 full column rank를 가진다면, $\Phi^{\intercal}\Phi$는 intervible할 꺼고, 이때 우리는 empirical risk의 minimizer를 선대에서 배운 걸로(혹은 걍 미분때려서) unique한 걸 확인 할 수 있고, 이를 OLS estimator라고 말할 것이다.

### 3.3.3. Closed-Form Solution

결론은 간단한데, 그냥 empirical risk 미분하면 normal equation 나오니까 full column rank이니까 넘기면 끝난다이긴 하다. 

## 3.4. Statistical Analysis of OLS

OLS에 대해 여러 분석을 할 건데, 세팅 방법이 두 가지가 있다. 
- Random Design: input과 output가 random일떄, 목표는 unseen test data로 일반화하는 것. (이건 fixed design 이후에 보자)
- Fixed design: Input은 고정, output이 random: 여기서의 목표는 observed input point드에 대한 좋은 prediction을 하는 것. (어떻게 보자면 label들의 noise를 제거하는 과정..으로도 볼 수 있겠다)

## 3.5 Fixed Design Setting

Fixed에서는 $\Phi$가 deterministic하니까, risk를 $\mathcal{R}(\theta)=\mathbb{E}_{y}[\frac{1}{n}\|y-\Phi \theta\|_{2}^{2}]$로 볼 수 있겠다. 그리고 variance matrix를 $\hat{\Sigma}=\frac{1}{n}\Phi^{\intercal}\Phi$라 볼 것이고 이는 invertible이다. 그렇다면 true parameter $\theta^{\ast}$가 존재해서 $y_{i}=\varphi(x_{i})^{\intercal}+\varepsilon_{i}$를 모든 $i \in [n]$에 대해 만족시킬 것이고, 이 때 Noise variable은 각자 독립이면서 $\mathbb{E}[\varepsilon_{i}]=0$, Variance는 $\sigma^{2}$로  놓게 할 것이다. 

한가지 눈치챌 수도 있는 점은, log-likelihood를 고려했을 때 Gaussian noise일때 OLS=MLE이긴 하다. (그냥 지나가는 상식..)

### 3.5.1. Statistical Properties of the OLS Estimator

이제 여러 가지 성질을 알아보자. 첫 번째는 $\mathcal{R}(\theta)=\sigma^{2}+\|\theta-\theta^{\ast}\|_{\hat{\Sigma}}^{2}$, 그리고 $\mathcal{R}^{\ast}=\sigma^{2}$인 점인데, 이는 $y=\Phi\theta^{\ast}+\varepsilon$ 으로 저리해서 풀어주면 끝난다. 

다음 정리는 그 유명한 bias-variance decomposition이다. 이거는 앞의 식에서 bayes risk 빼주고, $\mathbb{E}[\hat{\theta}]$ 기준으로 쪼개서, 중간항은 날라가기에 해결된다. 

다음 정리는 OLS가 unbiasedness(즉, 기댓값이 bayes estimator)를 지니고, variance가 $\frac{\sigma^{2}}{n}\hat{\Sigma}^{-1}$임을 보이는 것이다. 전자는 그냥 OLS에다가 평균 때리면 $\mathbb{E}[y]=\Phi\theta^{\ast}$이니까 끝나고, 분산도 그냥 error term에 대해서 covariance matrix 생각해서 expectation 내부로 집어넣으면 된다. 

다음 정리는 $\mathbb{E}[\mathcal{R}(\hat{\theta})]-\mathcal{R}^{\ast}=\frac{\sigma^{2}d}{n}$이라는 것인데, excess risk는 오로지 noise varaince, dimension, sample size에 의존함을 증명할 것이다. 이거는 내부가 상수임을 생각하ㅐ서 trace trick으로 $\text{tr}(\hat{\Sigma} \cdot \text{Var}(\hat{\theta}))$가 되고, 아까 구한 분산을 넣으면 끝난다. 

이를 바탕으로 관찰을 해보자. 일단 estimation error와 expressiveness(large $d$) 사이에는 tradeoff가 있고, $d/n$가 작을 떄만 excess risk는 작고, $d$가 오히려 가까우면 excess risk가 크고 잘 작동을 안 하기에, ridge penalty 같은 regularization 작업이 필요하다. 

## 3.6. Ridge Least-Squares Regression

그렇게 OLS는 high dimension에서 unstable/ill-posed이기에, ridge regression이 등장하였다. PCA로 dimension을 줄이는 방법도 있긴 한데, 일단 regularization인 ridge 방법을 살펴보도록 하자. 
결과적으로는, 앞에서 다뤘던 거에다가 $\lambda \|\theta\|_{2}^{2}$ 를 추가한 거에 대한 식을 최소화하는 값을 $\hat{\theta}_{\lambda}$ 라 볼 것이다. 이거의 장점은 이제 항상 well-defined이 되는 점이고, 복잡도를 $\lambda$ 로 조절한다는 것이다. 우선 이 경우의 estimator는 $\hat{\theta}_{\lambda}$가 $\frac{1}{n}(\hat{\Sigma}+\lambda I)^{-1}\Phi^{\intercal}y$로 나오고, 증명은 동일하게 gradient로 된다. $\lambda>0$인 경우에 대해 solution이 unique한 것이 가장 큰 장점이다. 

이때, 우리는 excess risk를 비슷하게 되게 복잡한 식으로 유도할 수 있다. Bias-variance decomposition을 열심히 써서, 앞에 했던 것처럼만 하면 충분히 유도할 수 있다. 

그런데, 이 bias-variance decomposition은 어떻게 $\lambda$가 tradeoff를 조절하는지를 보여주기도 한다. $\lambda=0$일 떄 bias는 $0$이고, 무한대로 가면 특정 값으로 수렴하며, Varaince는 $\lambda$가 커질수록 $0$으로 수렴한다. 

조금 색다른 접근으로는 effective degree of freedom이라는 걸 볼 수가 있는데, 결국 식이 의미하는 바는, $\lambda$에 따른 각 eigendirection의 contribution이다. (혹은 ridge에서 사용된 parameter의 soft count) 

그래도, excess risk에 대한 upper bound는 조금 더 간략한 식으로 구할 수가 있는데, covariance matrix을 직교대각화한 후 간단한 부등식을 이용하여 Bias와 Variance을 upper bound 시켜서, 산술기하 부등식을 쓰는 증명이다. 여기서 $R$을 각 matrix의 vector 중 최대 크기로 정의한다면, covariance matrix의 trace는 $R^{2}$로 bound된다. 그렇기에 우리가 알아낸 경우는, 이 경우 복잡도와 $d$의 관계성은 떨어진다: 오히려 $R$이나 covariance matrix의 trace가 더 강력한 정보를 내포하고 있다. 

당연히 Ridge bound와 OLS risk도 비교를 해볼 만한데, 결국 여기도 tradeoff가 있다. OLS의 risk 감소 속도가 더 빠르긴 하나, 상수에 따르면 이걸 원치 않는 경우도 올 수도 있다. 

마지막 질문. 그럼 $\lambda$를 어떻게 잡을까? 이거의 최적은 우리가 모르는 값을 쓰기에 힘들고, 여러 가지 실용적 방법이 존재하지만 여기선 다루진 않는다. 

## 3.8. Random Design Analysis

이제 Fixed Design은 충분히 보고도 남았으니, Random Design을 보자. 이제는 $x$ 조차도 random한 거고, 일단은 noise vector의 경우 $x$f랑 수직하다고 생각할 것이다. 이때 $\Sigma=\mathbb{E}[\varphi(x)\varphi(x)^{\intercal}]$을 noncentered covariance로 정의를 하고, population risk은 동일하게 정의할 것이다. 이건 당연히 well-specificed linar model이고, Bayes predictor은 우리 function class 안에 있을 것이다. 

이제 몇 가지 정리로 시작하자. 첫 번째는 $\mathcal{R}(\theta)-\mathcal{R}^{\ast}=\|\theta-\theta^{\ast}\|_{\Sigma}^{2}$인데, 앞과 증명이 똑같다. 

이제 fixed design의 covaraince matrix을 가져와서 생각한다면, 우리는 expected excess risk를 $\frac{\sigma^{2}}{n}\mathbb{E}[\text{tr}(\Sigma\hat{\Sigma}^{-1})]$ 으로 구할 수 있다. 이거의 증명은 앞의 정리를 바탕으로 error term 쓰고, trace trick 쓰면 된다. 한 가지는 $\Phi$에 대해 error term을 conditioning하는 점을 빠트리지 말 것. 

### 3.8.1 Gaussian Designs

만일 feature matrix의 row들이 Guassian Distribution을 따른다면, 되게 재밌는 성질이 나온다. Wishart matrix라는 걸 사용한다면, asympototic하게 $n$이 커지면 expected risk가 $\frac{\sigma^{2}d}{n}$ 로 수렴한다는 걸 볼 수 있다. 

그렇다면, Gaussianness 없이 저 trace를 조절할 수 있을까? 어떤 행렬에 대해 eigenvalue이 최솟값을 바탕으로, bound을 잡을 순 있다. 여기다가 Matrix Bernstein 부등식을 끼얹고, 적절한 moment condition을 부여한다면 우리는 $n$이 적당하게 클 떄, 좋은 확률로 minimum eigenvalue를 bound할 수 있고, 이걸로 결국 excess risk를 bound할 수가 있다. 

결론은 뭘까? $n$이랑 $d$가 비슷할 때 ill-conditioning은 저 trace 값을 증가시키고, ridge나 dimension reduction 등으로 이어지게 된다. 

# 4. Empirical Risk Minimization

앞에서 다뤘던 ERM을 좀 더 파보자. 예를 들어 empricial 0-1 loss를 최소화한다고 할 때, 이건 combinatorial search problem이기에 NP-hard이다. 그렇기에, discrete 조건을 relax를 해서 sign을 취하는 방법을 생각을 해보자. 

## 4.1. Convexification of the Risk

그러니까, score function $g:\mathcal{X} \rightarrow \mathbb{R}$ 이 있을 떄, 이거의 sign값만 딱 떼어서 생각을 해주는 건데, $0$인 경우는 어떠한 tie breaking rule을 써도 상관없다(어차피 measure zero라) 그리고 margin을 $u:=yg(x)$라 하자. 즉 매우 큰 $u$는 confident correct prediction을 의미하고, negative는 오류인 거다. 

그렇다면 이제 $g$에 대해 0-1 risk를 $\mathbb{P}(f(x) \neq y)$으로 볼 수 있는데, 생각해보면 이 경우는 $yg(x)<0$이라는 의미고, $g(x)=0$인 경우는 uniform하게 찍으니까 misclassification 확률이 0.5다. 그러면 

$\mathcal{R}(g)=\mathbb{E}[1\{g(x)\neq 0, yg(x)<0\}]+\mathbb{E}[1\{g(x)=0\}1\{f(x) \neq y\}]=\mathbb{E}[1\{yg(x)<0\}]+\frac{1}{2}\mathbb{E}[1\{g(x)=0\}]$으로 볼 수 있으니까, $\Phi_{0,1}(u)$를 음수면 $1$, 0이면 $1/2$, 양수면 $0$으로 정의해서, 이 함수에다가 $yg(x)$를 넣은 것의 기댓값으로 볼 수 있고, 이를 margin-based loss라 볼 것이다. 

문제점은, 이 친구는 discontinuous하고 nonconvex이기 때문에, ERM은 computationally intractable하다. 

### 4.1.1. Convex Surrogates

대안은 여기 있다. convex surrogate(hinge,logistic,square) 등으로 치환을 할 것이다. 이러면 convex이니까 optimization이 편해지고, smooth/Lipschitiz라 generalization analysis이 편하다. 각각을 살펴보자. 

Square loss인 경우는 $\Phi(yg(x))=(y-g(x))^{2}$이기에, 결국 least square regression을 label $\{-1,1\}$에 대해서 하는 것이다. 여기서 문제점은, $u>>1$이면, loss가 다시 커져서 large positive margin에 대해서 over-penalization을 하게 된다. 

Logistic loss는 $\Phi(u)=\log(1+e^{-u})$인데 $\sigma$가 sigmoid 함수일 때 $\Phi(yg(x))=-\log(\sigma(yg(x)))$으로 표현할 수 있다. 그렇다면 logistic model 아래에서는, risk는 negative conditoinal log-likelihood: $\mathcal{R}_{\Phi}(g)=\mathbb{E}[-\log p(y\mid x)]$ 로 볼 수 있다.

Hinge loss는 $\Phi(u)=\max(1-u,0)$,squared loss는 여기다 제곱을 취한 것이다. 둘 다 size 1의 margin buffer를 encourage하는 것이 특징. SVM과 연결된다. 

exponential loss는 $\phi(u)=e^{-u}$로, negative margin을 강하게 penalize한다. 

### 4.1.3. Conditional $\Phi$-risk and Classification Calibration

근데 surrogate까지는 알겠는데, surrogate risk를 최소화시키는 게 도움은 되는지.. 실제 0-1 risk는 Bayes optimum으로 보는지에 대한 질문이 있을 수 있다. 결론은, classification-calibrated surrogate들에 대해서는 가능하다. 

$\eta(x)$ 를 임의의 $x$에 대해서, $y=1$일 확률이라고 하자. 그렇다면 $\mathbb{E}[y\mid x]=2\eta(x)-1$일 것이며, Bayes 0-1 risk는 $\mathcal{R}^{\ast}=\mathbb{E}[\min(\eta(x),1-\eta(x))]=\frac{1}{2}\mathbb{E}[1-\mid2\eta(x)-1\mid]$ 이다. 여기에 부합하는 한 가지 classifier는 $2\eta(x)-1$ 의 sign을, 그러니까 $\mathbb{E}[y\mid x]$ 의 sign을 택하는 것이다. 당연히 이런 function은 무한히 많을 것이고, sign만 같으면 optimal할 것이다. 그렇다면 sign preserving $b$에 대해 $g(x)=b(2\eta(x)-1)$이라고 모든 꼴을 말할 수 있겠다. 

sanity check으로 square loss쓰면, 실제로 저게 됨을 확인할 수 있다. 그렇기에 least-square를 바탕으로 한 분류가 optimal population prediction을 낸다고 말할 수 있다. 

이제 메인으로 들어가자. Conditional $\Phi$-risk를 $C_{\xi}^{\Phi}(u)=\xi \Phi(u)+(1-\xi)\Phi(-u)$로 conditional $\Phi$-risk로 정의한다. 예를 들어, Conditional 0-1 risk는 $\Phi_{0-1}$을 넣어주면 된다. 

이걸 왜 꺼냈냐면, $\mathcal{R}_{\Phi}(g)=\mathbb{E}[C_{\eta(x)}^{\Phi}g(x)]$임을 이용하기 위해서다. 이는 tower property와 conditional surrogate risk의 정의에서 따라온다. 

0-1 conditional risk를 좀 더 살펴보자면, $\xi>\frac{1}{2}$ 일 때 이를 minimize하는 $u$는 양수, $\xi<\frac{1}{2}$ 인 경우는 반대로 $u$가 음수일 때가 minimizer가 된다. 

우리는 어떤 conditional surrgoate risk에 대해, $\Phi$가 classification-calibrated되었다는 것을 $\xi>1/2$인 것과 이 떄 $C_{\xi}^{\Phi}(u)$를 minimize하는 $u$는 전부 양수, 그리고 반대방향도 성립할 때 이렇게 말한다. 해석을 하자면, classification-calibrated된 경우는 surrogate risk를 써도 된다.

상당히 좋은 확인 방법은, classification-calibrated된 것과 $0$에서 미분 가능하고, $\Phi'(0)<0$인 경우가 동치이며, 이는 그냥 직접 미분해서 확인할 수 있다. 증명은 convexity를 바탕으로 0 근처에서 좌미분 우미분 하면 된다. 

이게 의미하는 바는, 이제 좋은 comparison function $H$가 존재해서, $\mathcal{R}(g)-\mathcal{R}^{\ast} \le H(\mathcal{R}_{\Phi}(g)-\mathcal{R}_{\Phi}^{\ast})$이 된다는 거가 있는데, 이 것의 의미는 다음 챕터에서. 

### 4.1.4. Relation between Risk and $\Phi$-risk



## 4.2. Risk Minimization Decomposition
## 4.3 Approximation Error
## 4.4. Estimation Error
### 4.4.1. Application of McDiarmid's Inequality
### 4.4.2. Easy case I: Quadratic Functions
### 4.4.3. Easy case II: Finite Number of Models
### 4.4.4. Beyond Finite Many Models through Covering Numbers
## 4.5. Rademacher Complexity
### 4.5.1. Symmetrization
### 4.5.2. Lipschitz-Continuous Losses
### TBH

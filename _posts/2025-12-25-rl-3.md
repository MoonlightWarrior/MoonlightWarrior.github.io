---
layout: post
title: RL 공부 - 3
date: 2025-12-27 00:00:00
permalink: /:title
categories: [RL, AI]
tag: ['2026']
description: Policy Gradients
comments: true
math: true
published: true
lang: ko
---

# On-policy policy gradient

우리의 첫 online RL algorithm을 공부해보자. 

매우 간단한 설명을 해보자면
- policy를 돌려서 data를 얻은 후
- policy를 얻은 data로 improve하는 과정 반복하기. 

Improve를 한다는 것의 정확은 의미가 뭘까? 결론적으로 우리는 trajectory가 reward를 최대한 많이 내는 방향으로 가고 싶으니, 일단 목표함수였던 expected sum of rewards에서 expectation을 완벽하게 구하진 못하니, $N$개 정도를 sample하는 것으로 근사하긴 할 꺼다. (즉, $J(\theta)=\mathbb{E}\_{\tau \sim p\_{\theta}(\tau)}[\sum\limits\_{t}r(s\_t,a\_t)] \simeq \frac{1}{N}\sum\limits_{i}\sum\limits_{t}r(s\_{i,t},a\_{i,t})$ )가 될 것이다. 

## 유도

일단 trajectory $\tau$의 reward 합을 $r(\tau)$으로 정의하자. 그렇다면 결국 우리는 $J(\theta)=\int p_{\theta}(\tau)r(\tau)d\tau$으로 볼 수 있는데, 로그의 미분과 미분과 적분 순서를 바꿀 수 있음을 가정한다면, 우리는 이 함수의 gradient를 $\mathbb{E}_{\tau \sim p\_{\theta}(\tau)}[\nabla\_{\theta}\log p\_{\theta}(\tau)r(\tau)]$으로 볼 수 있겠다. 여기서 $p\_{\theta}$의 의미를 고려해서 식을 좀 더 풀어쓰면 log likelihood들의 gradient들의 합에다가 reward function sum을 곱한 결과가 나온다. 

## 전체 알고리즘, 직관적 이해

즉, 
1. 기존 policy에서 sample trajectory을 많이 얻고, 
2. gradient를 구해서 (backward pass)
3. gradient descent를 하는 것이다. 이 과정을 REINFORCE algorithm, vanilla policy gradient이라고 한다. 

즉 behavior cloning의 gradient decent 꼴 생각해보면 사실 앞부분의 log-likelihood function 파트가 보이긴 한데, reward로 가중치가 주어진 것이다. 

즉 직관적으로
- high reward trajectory인 것의 likelihood를 늘리고
- loew reward trajectory인 것의 likelihood를 줄임으로써

몸소 trial-and-error를 실천한 것이 된다. 

## 개선방안 - casuality, baselines

다만 이것만 가지곤 부족할 순 있다. 일반적으로 policy gradient는 noisy/high-variance이기에, 한 가지 방안은 **casuality**를 고려하는 것이다. 즉 $t$의 policy behavior는 그전 reward에 영향을 안 주기에, sum of future reward를 대신 reward sum으로 써 주는 것이다. 

이래도 여전히 부족할 순 있는데, 물론 noisy 문제도 있긴 한데, reward scale에 sensitive하다는 점도 있다. 여기서 확률이 합이 1임에 근거해서 미분을 하면 $0$임을 이용한다면, 어떤 baseline(: 즉 trajectory들의 평균)을 빼서 reward로 책정하면 어떨까? 그럼 평균 이상의 잘 한 이들은 reward가 양수, 아닌 경우가 음수가 되는 것이다. (게다가, gradient의 variance를 줄일 수 있다!) 뭐 그래도 여전히 noisy, high-variance임은 있을 수 있어서, 보통 policy gradient은 dense reward, large batch인 경우 잘 작동한다. 

게다가, 현재의 알고리즘대로면, $N \times T$ backward pass를 해서 각각을 구하는 건 inefficient하기도, 이때 gradient가 동일한 surrogate objective를 쓰는 방법이 존재한다. 이때 이 log-likelihood는 discrete action policy는 cross-entropy가 되며, gaussian policy의 경우 squared error가 된다. 

# Off-policy policy graident

아직 남은 문제가 뭐가 있을까? 우리는 모든 gradient step당 다시 data를 다시 recollect해야 한다. (즉, on-policy다)

## Importance sampling

간단한 적분 수식 변형을 통해 $\mathbb{E}\_{x \sim p(x)}[f(x)]=\mathbb{E}\_{x \sim q(x)}[\frac{p(x)}{q(x)}f(x)]$ 가 됨을 알 수 있다. 이 자리에 previous policy의 sample을 다시 대입한다면, 이걸 바탕으로 그다음 policy를 같은 sample를 가지고 구할 수 있긴 하다. 다만 거대한 곱셈항이 생기니, expectation of time step을 쓰면 explode/vanish할 걱정을 안해도 된다. 그렇지만 구하기가 어렵고 그냥 state가 각 policy에 뜰 확률을 $1$으로 approximate한다면 조건부확률로 꼴이 나온다. 

그렇다면 policy가 sample하기 전에 많이 바뀐다면? 그럼 gradient estimate가 아무래도 예측률이 떨어진다..

## KL constraints

그렇다면 policy가 gradient update할 동안 너무 벌어지지 않게 하려면? 두 정책의 KL-divergence가 현재 state가 기존 policy를 따를 때의 기댓값을 bound시키면 된다. 
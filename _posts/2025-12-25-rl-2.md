---
layout: post
title: RL 공부 - 2
date: 2025-12-26 00:00:00
permalink: /:title
categories: [RL, AI]
tag: ['2026']
description: Imitation Learning
comments: true
math: true
published: true
lang: ko
---

# Imitation Learning의 목표

앞서 말했다시피 imitation learning의 목표는 expert가 준 trajectory들: $\mathcal{D}:=\{(s_1,a_1,\cdots,s_T)\}$ 의 집합을 어떤 unknown policy $\pi_{\text{expert}}$ 한테서 배웠을 떄, 이를 expert policy의 수준으로 mimic하는 policy $\pi_{\theta}$를 공부하는 것이다. 

# Imitation Learning

## Version 0

우리의 첫 방식은 다음과 같다. 단순히 MSE를 minimize하면 된다는 생각이다. 
즉 $\min\limits_{\theta}\frac{1}{\vert \mathcal{D} \vert}\sum\limits_{(s,a) \in \mathcal{D}}\|a-\hat{a}\|^{2}$ where $a=\pi_{\theta}(s)$(deterministic), 즉 expert의 action들에 대해 지도학습을 돌려서 policy $\pi_{\theta}$를 얻은 것이다!

신경망의 관점에서는, forward pass를 하고, mini batch를 sample한 후, loss를 구해서 backpropagation으로 parameter를 adjust하면 된다. 그리고 이걸 제일 맘에 드는 optimizer (ex. SGD)로 policy를 optimize하면 된다.  

...이게 맞을까?

### 문제점?

우선 주어진 data에 대해서, policy가 nondeterministic일 수 있다. 주어진 경로를 보고 운전자가 예를 들어 앞으로 갈 수도 있고, 왼쪽으로 갈 수도 있으니. 그리고 $l_2$ regression으로 훈련시키면 무슨 문제가 생길까? 아예 중간으로 가서..사고가 생길 수 있다. (mean action의 문제점: 오히려 distribution에서는 확률이 더 낮을 수 있다) (어떻게 보자면, KNN처럼 느낄 수도 있겠다.)

이게 실제로 많이 일어나는 사례인가면, 그렇다. 이건 항상 여러 사람한테 data가 retrieve될 때 생기는 일이다. 그렇다면, mean보단 distribution 자체를 배울 수 있는 방법은 없을까? 여기서, 신경망으로 distribution을 학습하는 것이 나온다. 

즉 우리는 neural network가 probability distribution의 parameter들을 내놓길 원하는 것이다. 예를 들어 1D discrete action인 경우, 각각의 probability를 원하는 거고, 이건 maximally expressive하다. (이걸로 action을 다 표현할 수 있으니) 그러나 continuous action의 경우는 Gaussian Distribution을 요구할 수도 있긴 한데, 이건 variance 하나 더 해도, 그렇게 expressive하진 못하다. (결국 gaussian 분포에 갇혀 있으니..)

여기서 generative modeling을 쓸 수 있을까? (ex. image distribution, autoregressive models는 이미 꽤나 쓰이고 있다.) 예를 들자면
- Mixture of gaussians를 써서, 각 정규분포의 mean과 variance, 그리고 각각의 가중치를 얻는 방법. 일반적으로 number of gaussians는 hyperparameter로 취급하면 된다. 
- Discretize + Autoregressive: 일단 연속분포를 적당히 bin들로 쪼개서 discretize시킨 다음에, 여기에 대해서 Autoregressive를 돌리자. 즉 첫 action에 대한 분포를 예측하고, 그걸 선택했으면 그다음 action을 조건부로 구하고, ..., 
- Diffusion: 자세히는 안 다뤘다. 일단 간단하게만 설명하자면, 매우 noisy한 것에서 시작해서, denoise하는 방식으로 diffusion이 진행된다. 3가지 방식 중에서 expressibility가 가장 뛰어나다. 

## Version 1

정리를 해보자면, 

0. Expert들의 demonstration $\mathcal{D}$ 모으기!
1. expert action들에 대해 generative model를 train하기! 즉, $\min\limits\_{\theta}-\mathbb{E}\_{(s,a) \sim \mathcal{D}}[\log \pi_{\theta}(a\vert s)]$ 을 하는 것이다. (즉, log-likelihood를 maximize하는 것)
2. 이걸 바탕으로 배운 정책 $\pi_{\theta}$를 deploy하는 것

실제로 보면, Expressibility가 높은 diffusion이 GMM이나 L1-minimization 등의 방법론보다 좋은 성과를 보임이 여러 사례에 나왔다. 

## 현재까지의 요약

- Expressive policy class를 demonstration dataset에 대해 generative modeling으로 train 하는 것. 
- 알고리즘은 offline이다. (즉, 새로운 data를 더 쓰지 않는다는 의미, 반면 online은 learned policy의 data를 obtain해서 쓰는 것)
- reward function을 정의를 필요하지 않는 대신, reliablity를 위해서 많은 양의 데이터가 필요할 수 있다. 

# Imitation Learning의 문제점

**오차의 stacking.**.
- 예상한 action은 당연히 그다음 state를 예측하고, error의 누적이 data distribution에서 더 멀어지게 한다. 

## 해결책?

그렇다면 어떻게 해결해야 할까?
1. 엄청나게 많은 demo data를 가져와서 최선을 기대하기. 
2. corrective behavior data를 모으기. (?)

2를 좀 자세히 보자. 
이 방법을 DAgger(dataset aggregation)이라고 한다. 
1. 우선 현재 배운 policy를 roll out한다. 
2. 해당 trajectory에 대해, 각 visited state들에 대한 expert action을 query한다. 
3. 해당 찾은 correction용 data를 기존 data에 추가한 후, 
4. 다시 policy를 update한다. 

이 방식은 expert한테 데이터를 구하는 data-efficient way이다만, agent가 control를 가진 시점 expert를 query하기 어려울 수도 있다. 그럼 다른 방식이 있을까?

두 번째 방식은 human gated DAgger이라고 부른다. 
1. 우선 현재 배운 policy를 roll out한다. 
2. expert는 policy가 error를 낸 시점 $t$에 intervene한다. 
3. Expert는 그 시점부터의 demonstration을 제공한다. 
4. 이러한 demostration에서 얻은 결과를 data에 추가한다. 
5. 다시 policy를 update한다. 

이 방식은 correction을 provide하는데 있어 더 practical한 interface를 주지만, 어떤 domain에서 mistake를 잡기 어려울 수도 있다. 

질문: 그럼 이런 개입이 필요한 시점은 언제인가?: 전문가 행동과의 차이가 어떤 threshold를 넘었을 때? 그러면 전문가 행동을 미리 알아야 하니 완전한 "gated"는 아니다. 그렇다면 모델의 variance가 어떤 threshold를 넘었을 때? (즉, 자신 없을 때). 이러면 전문가 없이 판단 가능하지만, 자신이 없어도 안전할 순 있다. 혹은 아예 intervention problem 자체를 classification problem으로 인식하여 학습하는 방식도 가능은 하다만, 여기부턴 2차적 문제. 

# Demonstration을 모으는 방법

좀 처음으로 돌아가서, 결론적으로 $\mathcal{D}$를 어떻게 얻을까? 운전이나 메세지 등 쉬운 demonstration도 있긴 한데, robotics로 가면.. 조금 어려워진다. 예를 들어 kinesthetic teaching의 경우 인터페이스는 쉬워도 사람이 있어야 하고..그리고 사람을 모방한 로봇은 모습, 자유도, 물리적 능력이 모두 다르기에 embodiment gap이 존재한다. 그래도, exploration 관점에선 좋을 수 있다. 

# 결론

처음에는 Behavior Cloning(offline)을 배웠고, 그 후 오차에 대한 대처로 DAgger, HG-DAgger등의 method를 배웠다.(online)

실제론 많은 방식은 reinforcement learning과 imitiation learning을 결합해서 사용한다!